{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-08T19:46:27.262846Z",
     "iopub.status.busy": "2022-02-08T19:46:27.262274Z",
     "iopub.status.idle": "2022-02-08T19:46:27.29473Z",
     "shell.execute_reply": "2022-02-08T19:46:27.293513Z",
     "shell.execute_reply.started": "2022-02-08T19:46:27.262694Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#      for filename in filenames:\n",
    "#          print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:46:30.304209Z",
     "iopub.status.busy": "2022-02-08T19:46:30.303723Z",
     "iopub.status.idle": "2022-02-08T19:46:38.512989Z",
     "shell.execute_reply": "2022-02-08T19:46:38.511944Z",
     "shell.execute_reply.started": "2022-02-08T19:46:30.304155Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "import string \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import Conv2D , MaxPooling2D , Dense  , BatchNormalization , LSTM  , Concatenate , Input,Embedding,Flatten\n",
    "from tensorflow.keras.applications import resnet_v2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:46:38.515298Z",
     "iopub.status.busy": "2022-02-08T19:46:38.515027Z",
     "iopub.status.idle": "2022-02-08T19:46:52.521459Z",
     "shell.execute_reply": "2022-02-08T19:46:52.520386Z",
     "shell.execute_reply.started": "2022-02-08T19:46:38.515265Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the data \n",
    "numric_train_data = pd.read_parquet('C:/Users/DELL/Downloads/kaggle-pog-series-s01e01/train.parquet')\n",
    "numric_test_data = pd.read_parquet('C:/Users/DELL/Downloads/kaggle-pog-series-s01e01/test.parquet')\n",
    "\n",
    "# identify key columns \n",
    "text_columns = ['title' , 'tags' ,'description' ]\n",
    "categrical_data = ['categoryId' , 'comments_disabled' , 'ratings_disabled']\n",
    "numric_columns = ['view_count' , 'likes' , 'dislikes' , 'comment_count' , 'duration_seconds']\n",
    "\n",
    "# function to process the data\n",
    "def process_data(data) :\n",
    "    data['duration_seconds'] = data['duration_seconds'].fillna(data['duration_seconds'].mean())\n",
    "    data[categrical_data] = data[categrical_data].astype('category')\n",
    "    cats_columns = pd.get_dummies(data[categrical_data]) \n",
    "    data = data.drop(categrical_data , axis = 1)\n",
    "    data = pd.concat([data , cats_columns] , axis = 1)\n",
    "    data = data.drop([ 'has_thumbnail' , 'channelId' , 'channelTitle' , 'id'] , axis = 1)\n",
    "    data['trending_date'] = pd.to_datetime(data['trending_date'])\n",
    "    return data\n",
    "\n",
    "train_data_updated = process_data(numric_train_data)\n",
    "test_data_updated = process_data(numric_test_data)\n",
    "\n",
    "# loading the images paths and \n",
    "from pathlib import Path\n",
    "main = Path('C:/Users/DELL/Downloads/kaggle-pog-series-s01e01/thumbnails')\n",
    "images_paths = list(main.glob(r'**/*.jpg'))\n",
    "images_id = list(map(lambda x: os.path.split(os.path.split(x)[1])[1], images_paths))\n",
    "images_S = pd.Series(images_paths , name='path').astype('str')\n",
    "id_S = pd.Series(images_id , name='id')\n",
    "images_df = pd.concat([images_S , id_S] , axis = 1 )\n",
    "images_df['id'] = images_df['id'].apply(lambda X : X.replace('.jpg' , ''))\n",
    "\n",
    "# merge the two dataframe to map each row to his image\n",
    "train_data_frame = images_df.merge(train_data_updated , how ='inner' , left_on ='id' , right_on ='video_id')\n",
    "test_data_frame = images_df.merge(test_data_updated , how ='inner' , left_on ='id' , right_on ='video_id')\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_updated[numric_columns] = scaler.fit_transform(train_data_updated[numric_columns]) \n",
    "\n",
    "test_data_frame['description'] = test_data_frame['description'].fillna('empty')\n",
    "train_data_frame['description'] = train_data_frame['description'].fillna('empty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:46:52.524303Z",
     "iopub.status.busy": "2022-02-08T19:46:52.523593Z",
     "iopub.status.idle": "2022-02-08T19:46:52.534598Z",
     "shell.execute_reply": "2022-02-08T19:46:52.533729Z",
     "shell.execute_reply.started": "2022-02-08T19:46:52.524247Z"
    }
   },
   "outputs": [],
   "source": [
    "# some text processing \n",
    "def remove_punc(text): \n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_chars(text):\n",
    "    return re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "def remove_stop_words(text):\n",
    "    cleaned = []\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    sp = nltk.stem.PorterStemmer()\n",
    "    for word in text :\n",
    "        if word not in set(stopwords.words('english')) : \n",
    "            word = sp.stem(word)\n",
    "            cleaned.append(word)\n",
    "    return ' '.join(cleaned)\n",
    "def cleaning(text):\n",
    "#     text = remove_html(text)\n",
    "    text = remove_chars(text)\n",
    "    text = remove_stop_words(text)\n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:46:52.539012Z",
     "iopub.status.busy": "2022-02-08T19:46:52.538265Z",
     "iopub.status.idle": "2022-02-08T19:53:04.877619Z",
     "shell.execute_reply": "2022-02-08T19:53:04.87685Z",
     "shell.execute_reply.started": "2022-02-08T19:46:52.538952Z"
    }
   },
   "outputs": [],
   "source": [
    "# take subset of the data because of the computional cost # you can change it\n",
    "short_data_frame = train_data_frame.iloc[:5000 ,:]\n",
    "\n",
    "# applying text process in train and test data\n",
    "for col in text_columns : \n",
    "    short_data_frame[col] = short_data_frame[col].apply(cleaning)\n",
    "    test_data_frame[col] = test_data_frame[col].apply(cleaning)\n",
    "    \n",
    "    \n",
    "max_f = 10000\n",
    "max_len = 300\n",
    "text_features = {}\n",
    "\n",
    "from tensorflow.keras.preprocessing import text , sequence \n",
    "tock = text.Tokenizer(num_words = max_f)\n",
    "for col in text_columns :\n",
    "    tock.fit_on_texts(short_data_frame[col])\n",
    "    text_features[col] = tock.texts_to_sequences(short_data_frame[col])\n",
    "    text_features[col] = sequence.pad_sequences(text_features[col] , maxlen = max_len)\n",
    "text_train_data = np.concatenate([text_features[text_columns[0]] , text_features[text_columns[1]],text_features[text_columns[2]]] ,axis =1)\n",
    "\n",
    "test_text_features = {}\n",
    "for col in text_columns :\n",
    "    tock.fit_on_texts(test_data_frame[col])\n",
    "    test_text_features[col] = tock.texts_to_sequences(test_data_frame[col])\n",
    "    test_text_features[col] = sequence.pad_sequences(test_text_features[col] , maxlen = max_len)\n",
    "text_test_data = np.concatenate([test_text_features[text_columns[0]] , test_text_features[text_columns[1]],test_text_features[text_columns[2]]] ,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:53:04.879833Z",
     "iopub.status.busy": "2022-02-08T19:53:04.879277Z",
     "iopub.status.idle": "2022-02-08T19:53:05.876835Z",
     "shell.execute_reply": "2022-02-08T19:53:05.875705Z",
     "shell.execute_reply.started": "2022-02-08T19:53:04.879785Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and process images\n",
    "\n",
    "train_img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function = tf.keras.applications.resnet_v2.preprocess_input\n",
    ")\n",
    "test_img_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    ")\n",
    "X_img_train = train_img_generator.flow_from_dataframe(\n",
    "    dataframe = short_data_frame , x_col = 'path' ,\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode=None,\n",
    "    batch_size=32,\n",
    "    shuffle= False , \n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "X_img_test = test_img_generator.flow_from_dataframe(\n",
    "    dataframe = test_data_frame ,x_col = 'path' , \n",
    "    target_size = (244 ,244 ) , color_mode = 'rgb' , \n",
    "    class_mode = None , batch_size = 32 ,\n",
    "    shuffle = False , \n",
    "    seed = 42 \n",
    ")\n",
    "\n",
    "#convert images into numpy to avoid adapting error while training the model \n",
    "X_img_train=np.concatenate([X_img_train.next() for i in range(X_img_train.__len__())])\n",
    "X_img_test = np.concatenate([X_img_test.next() for i in range(X_img_test.__len__())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:58:14.87154Z",
     "iopub.status.busy": "2022-02-08T19:58:14.870725Z",
     "iopub.status.idle": "2022-02-08T19:58:14.95801Z",
     "shell.execute_reply": "2022-02-08T19:58:14.957267Z",
     "shell.execute_reply.started": "2022-02-08T19:58:14.87149Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the numric data \n",
    "numrical_data  =short_data_frame[[ 'categoryId_1',\n",
    "       'categoryId_2', 'categoryId_10',  'categoryId_17',\n",
    "        'categoryId_20', 'categoryId_22', 'categoryId_23',\n",
    "       'categoryId_24', 'categoryId_25', 'categoryId_26', 'categoryId_27',\n",
    "       'categoryId_28', 'comments_disabled_False',\n",
    "       'comments_disabled_True', 'ratings_disabled_False',\n",
    "       'ratings_disabled_True','duration_seconds']]\n",
    "numrical_data['publish_year' ] = short_data_frame['publishedAt'].dt.year\n",
    "numrical_data['trend_year' ] = short_data_frame['trending_date'].dt.year\n",
    "numrical_data['publish_month' ] = short_data_frame['publishedAt'].dt.month\n",
    "numrical_data['trend_month' ] = short_data_frame['trending_date'].dt.month\n",
    "numrical_data['publish_day' ] = short_data_frame['publishedAt'].dt.day\n",
    "numrical_data['trend_day' ] = short_data_frame['trending_date'].dt.day\n",
    "numrical_data = np.array(numrical_data)\n",
    "numrical_data = tf.cast(numrical_data , tf.float32)\n",
    "\n",
    "numrical_test_data = test_data_frame[[ 'categoryId_1',\n",
    "       'categoryId_2', 'categoryId_10', 'categoryId_17',\n",
    "    'categoryId_20', 'categoryId_22', 'categoryId_23',\n",
    "       'categoryId_24', 'categoryId_25', 'categoryId_26', 'categoryId_27',\n",
    "       'categoryId_28', 'comments_disabled_False',\n",
    "       'comments_disabled_True', 'ratings_disabled_False',\n",
    "       'ratings_disabled_True','duration_seconds']]\n",
    "numrical_test_data['publish_year' ] = test_data_frame['publishedAt'].dt.year\n",
    "numrical_test_data['trend_year' ] = test_data_frame['trending_date'].dt.year\n",
    "numrical_test_data['publish_month' ] = test_data_frame['publishedAt'].dt.month\n",
    "numrical_test_data['trend_month' ] = test_data_frame['trending_date'].dt.month\n",
    "numrical_test_data['publish_day' ] = test_data_frame['publishedAt'].dt.day\n",
    "numrical_test_data['trend_day' ] = test_data_frame['trending_date'].dt.day\n",
    "numrical_test_data = np.array(numrical_test_data)\n",
    "numrical_test_data = tf.cast(numrical_test_data , tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:58:22.016726Z",
     "iopub.status.busy": "2022-02-08T19:58:22.016116Z",
     "iopub.status.idle": "2022-02-08T19:58:22.021134Z",
     "shell.execute_reply": "2022-02-08T19:58:22.020469Z",
     "shell.execute_reply.started": "2022-02-08T19:58:22.016661Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = np.array(short_data_frame['target']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_feature_extractor = resnet_v2.ResNet152V2(include_top = False , weights ='imagenet' , input_shape=(224 ,224 ,3))\n",
    "image_feature_extractor.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T19:58:25.286502Z",
     "iopub.status.busy": "2022-02-08T19:58:25.285872Z",
     "iopub.status.idle": "2022-02-08T19:58:41.189192Z",
     "shell.execute_reply": "2022-02-08T19:58:41.188207Z",
     "shell.execute_reply.started": "2022-02-08T19:58:25.286446Z"
    }
   },
   "outputs": [],
   "source": [
    "# the model \n",
    "\n",
    "image_feature_extractor = resnet_v2.ResNet152V2(include_top = False , weights ='imagenet' , input_shape=(224 ,224 ,3))\n",
    "image_feature_extractor.trainable=False\n",
    "\n",
    "# img layers \n",
    "img_input = Input((224,224,3))\n",
    "img_features = image_feature_extractor(img_input)\n",
    "img_features = tf.keras.layers.Flatten()(img_features) \n",
    "img_features = Dense(256 , activation ='relu')(img_features)\n",
    "img_features = BatchNormalization()(img_features)\n",
    "#######\n",
    "\n",
    "# numric layers \n",
    "numric_input = Input((30,))\n",
    "numric_features = Dense(256 , activation='relu')(numric_input)\n",
    "numric_features = Dense(256 , activation='relu')(numric_features)\n",
    "numric_features = BatchNormalization()(numric_features)\n",
    "numric_features = Dense(128 , activation='relu')(numric_features)\n",
    "numric_features = Dense(128 , activation='relu')(numric_features)\n",
    "numric_features = BatchNormalization()(numric_features)\n",
    "#####\n",
    "\n",
    "# text_layers \n",
    "text_input = Input((900,))\n",
    "embed =Embedding(max_f , 100 )(text_input)\n",
    "lstm = LSTM(50)(embed)\n",
    "text_features =  Dense(265 , activation='relu')(lstm)\n",
    "text_features =  Dense(128 , activation='relu')(text_features)\n",
    "text_features = BatchNormalization()(text_features)\n",
    "#######\n",
    "\n",
    "\n",
    "cocat = Concatenate()([img_features,numric_features ,text_features])\n",
    "X = Dense(265 , activation = 'relu')(cocat)\n",
    "X = Dense(128 , activation = 'relu')(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Dense(64 , activation ='relu')(X)\n",
    "output = Dense(1)(X)\n",
    "model = Model([ img_input,numric_input , text_input] , output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:02:19.680418Z",
     "iopub.status.busy": "2022-02-08T20:02:19.67986Z",
     "iopub.status.idle": "2022-02-08T20:02:19.707173Z",
     "shell.execute_reply": "2022-02-08T20:02:19.705754Z",
     "shell.execute_reply.started": "2022-02-08T20:02:19.680379Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam' , loss =['mae', 'mae' ,'mae'] , metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-08T20:02:21.393923Z",
     "iopub.status.busy": "2022-02-08T20:02:21.393253Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit( [ X_img_train[:4000], numrical_data[:4000], text_train_data[:4000]] ,labels[:4000] , validation_data = ([X_img_train[4000:], numrical_data[4000:], text_train_data[4000:]] , labels[4000:]) , epochs =50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_img_test , text_test_data , numrical_test_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
